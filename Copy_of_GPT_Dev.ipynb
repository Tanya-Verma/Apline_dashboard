{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPFsLqJhyRjGa9TkSJfaIJQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tanya-Verma/Apline_dashboard/blob/main/Copy_of_GPT_Dev.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1N5bAaKEOJiS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99fe350e-442c-4510-d654-73561f526aa0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-02-02 07:23:07--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.2’\n",
            "\n",
            "\rinput.txt.2           0%[                    ]       0  --.-KB/s               \rinput.txt.2         100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2026-02-02 07:23:07 (21.4 MB/s) - ‘input.txt.2’ saved [1115394/1115394]\n",
            "\n",
            "length of dataset in characters:  1115394\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch # we use PyTorch: https://pytorch.org\n",
        "\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(\"length of dataset in characters: \", len(text))\n",
        "\n",
        "print(text[:1000])\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7cda345"
      },
      "source": [
        "## Implement `get_batch` Function\n",
        "\n",
        "### Subtask:\n",
        "Implement the `get_batch` function, which will randomly sample batches of data from either the training or validation set. This function is crucial for efficiently feeding data to the model during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8c4bcdea",
        "outputId": "34de53d5-d26c-4293-81b5-fd0e89733ab8"
      },
      "source": [
        "print(\"Generating 200 new tokens from the model:\")\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(generate(m, context, max_new_tokens=200)[0].tolist()))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating 200 new tokens from the model:\n",
            "\n",
            "Or my liveft ous\n",
            "Ats trayinem thou, that yous me mordn.\n",
            "\n",
            "LAnm; dithith alld se.\n",
            "\n",
            "LAPThat ad dep west, his my me:\n",
            "Py proodesss eentle cod the thaven, tuad hat hisBought day him thy that ould chald delf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88c556a6"
      },
      "source": [
        "## Define Toy Example Variables and Dummy Model\n",
        "\n",
        "### Subtask:\n",
        "Define `B`, `T`, `C`, `head_size`, and `xbow2` for toy examples. Also, provide a dummy PyTorch model (`m`) and optimizer to make the existing code snippets runnable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "291f5f5b",
        "outputId": "8c965afa-4a5e-4f82-bb28-62a5dd4d7a62"
      },
      "source": [
        "B, T, C = 4, 8, 32 # batch, time, channels (embedding dimension)\n",
        "head_size = 16\n",
        "xbow2 = torch.zeros((B,T,C)) # Dummy xbow2 for testing purposes\n",
        "\n",
        "# Dummy PyTorch model and optimizer to make snippets runnable\n",
        "m = torch.nn.Linear(C, C) # A simple linear layer as a placeholder model\n",
        "optimizer = torch.optim.Adam(m.parameters()) # A dummy optimizer\n",
        "\n",
        "print(f\"B (batch size): {B}\")\n",
        "print(f\"T (sequence length): {T}\")\n",
        "print(f\"C (channels/embedding dimension): {C}\")\n",
        "print(f\"head_size: {head_size}\")\n",
        "print(f\"Dummy model 'm' created: {m}\")\n",
        "print(f\"Dummy optimizer created: {optimizer}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "B (batch size): 4\n",
            "T (sequence length): 8\n",
            "C (channels/embedding dimension): 32\n",
            "head_size: 16\n",
            "Dummy model 'm' created: Linear(in_features=32, out_features=32, bias=True)\n",
            "Dummy optimizer created: Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    decoupled_weight_decay: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.001\n",
            "    maximize: False\n",
            "    weight_decay: 0\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27c0d3a9",
        "outputId": "35904584-ff4a-4af1-b1bd-75e7084b6987"
      },
      "source": [
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "# Test the get_batch function\n",
        "xb, yb = get_batch('train')\n",
        "print(\"Inputs (xb) shape:\", xb.shape)\n",
        "print(\"Targets (yb) shape:\", yb.shape)\n",
        "print(\"First batch inputs:\", xb[0])\n",
        "print(\"First batch targets:\", yb[0])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs (xb) shape: torch.Size([4, 8])\n",
            "Targets (yb) shape: torch.Size([4, 8])\n",
            "First batch inputs: tensor([45,  1, 57, 53,  1, 61, 47, 57])\n",
            "First batch targets: tensor([ 1, 57, 53,  1, 61, 47, 57, 43])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28066796"
      },
      "source": [
        "# Task\n",
        "Define the `encode` and `decode` functions, set `block_size` and `batch_size`, implement `get_batch`, define `B`, `T`, `C`, `head_size`, and `xbow2` for toy examples, and provide a dummy PyTorch model (`m`) and optimizer to make the existing code snippets runnable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c02d7c63"
      },
      "source": [
        "## Define Tokenization Functions\n",
        "\n",
        "### Subtask:\n",
        "Implement the `encode` and `decode` functions based on the `stoi` (string to integer) and `itos` (integer to string) mappings, which are crucial for processing the text data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64e977fa"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires defining `itos`, `encode`, and `decode` functions. I will start by creating the `itos` dictionary from the existing `stoi` dictionary and then implement the `encode` and `decode` functions as specified.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "df03fd64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "075e0a40-0199-49fa-a9a3-2bbf0e5db00b"
      },
      "source": [
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "\n",
        "def encode(s):\n",
        "    return [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "\n",
        "def decode(l):\n",
        "    return ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "print(\"itos dictionary created successfully.\")\n",
        "print(\"encode function defined.\")\n",
        "print(\"decode function defined.\")\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(f\"Data tensor created with shape: {data.shape}\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "itos dictionary created successfully.\n",
            "encode function defined.\n",
            "decode function defined.\n",
            "Data tensor created with shape: torch.Size([1115394])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc86bbc9"
      },
      "source": [
        "## Set Hyperparameters and Split Data\n",
        "\n",
        "### Subtask:\n",
        "Define `block_size` and `batch_size`. Split the `data` tensor into training and validation sets based on a 90/10 ratio.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04dca3a6"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires defining `block_size` and `batch_size`, and splitting the `data` tensor into training and validation sets. I will create a code block to accomplish these steps as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cba31be",
        "outputId": "e9b48007-2bec-4cb1-c38a-50dac5b74b77"
      },
      "source": [
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "batch_size = 4 # how many independent sequences will we process in parallel?\n",
        "\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "print(f\"block_size set to: {block_size}\")\n",
        "print(f\"batch_size set to: {batch_size}\")\n",
        "print(f\"Training data length: {len(train_data)}\")\n",
        "print(f\"Validation data length: {len(val_data)}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "block_size set to: 8\n",
            "batch_size set to: 4\n",
            "Training data length: 1003854\n",
            "Validation data length: 111540\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9378f04"
      },
      "source": [
        "# Task\n",
        "Implement token and positional embedding layers to convert input tokens into vector representations suitable for a transformer model, using `nn.Embedding` for tokens and `nn.Parameter` for positional encodings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29a1d2f7"
      },
      "source": [
        "## Implement Embedding Layers\n",
        "\n",
        "### Subtask:\n",
        "Create token and positional embedding layers to convert input tokens into rich vector representations suitable for the transformer model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7129dada"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires implementing token and positional embedding layers. I will define the vocabulary size, and then instantiate the `nn.Embedding` for token embeddings and `nn.Parameter` for positional embeddings as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "416b52e1",
        "outputId": "b17bf0b3-41a9-4597-dba0-f8c5da1d4e04"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "vocab_size = len(chars)\n",
        "\n",
        "token_embedding_table = nn.Embedding(vocab_size, C)\n",
        "position_embedding_table = nn.Parameter(torch.zeros(block_size, C))\n",
        "\n",
        "print(f\"Vocab size: {vocab_size}\")\n",
        "print(f\"Token embedding table created with shape: {token_embedding_table.weight.shape}\")\n",
        "print(f\"Positional embedding table created with shape: {position_embedding_table.shape}\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 65\n",
            "Token embedding table created with shape: torch.Size([65, 32])\n",
            "Positional embedding table created with shape: torch.Size([8, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bf18754"
      },
      "source": [
        "## Implement Attention Head\n",
        "\n",
        "### Subtask:\n",
        "Implement a single self-attention head, including the creation of query, key, and value linear layers, and the masked attention mechanism to prevent information leakage from future tokens.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59da835b"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires implementing the `Head` class for a single self-attention head, including its `__init__` and `forward` methods, as specified in the instructions. This code block will define the class with linear layers, attention calculation, masking, softmax, and dropout.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd399775",
        "outputId": "13995922-badc-474b-e01f-8450a0a3c55a"
      },
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(C, head_size, bias=False)\n",
        "        self.query = nn.Linear(C, head_size, bias=False)\n",
        "        self.value = nn.Linear(C, head_size, bias=False)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape # Unpack dimensions from input x\n",
        "        k = self.key(x)   # (B, T, head_size)\n",
        "        q = self.query(x) # (B, T, head_size)\n",
        "        v = self.value(x) # (B, T, head_size)\n",
        "\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5 # (B, T, head_size) @ (B, head_size, T) -> (B, T, T)\n",
        "\n",
        "        tril = torch.tril(torch.ones(T, T))\n",
        "        wei = wei.masked_fill(tril == 0, float('-inf')) # (B, T, T)\n",
        "        wei = torch.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "\n",
        "        # perform the weighted aggregation of the values\n",
        "        out = wei @ v # (B, T, T) @ (B, T, head_size) -> (B, T, head_size)\n",
        "        return out\n",
        "\n",
        "print(\"Head class defined successfully.\")\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Head class defined successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57c7e79c"
      },
      "source": [
        "## Implement Multi-Head Attention\n",
        "\n",
        "### Subtask:\n",
        "Combine multiple attention heads into a Multi-Head Attention module. This involves splitting and concatenating outputs from individual heads and a final linear projection.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f5ec89d"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires implementing the `MultiHeadAttention` class by combining multiple `Head` instances, adding a projection layer, and a dropout layer, as specified in the instructions. This code block will define the class and its methods.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3b14691",
        "outputId": "4bd57bd6-7c04-4118-e54a-14cd6f366cb1"
      },
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" Multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(head_size * num_heads, C)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "print(\"MultiHeadAttention class defined successfully.\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MultiHeadAttention class defined successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d03bd5c"
      },
      "source": [
        "## Implement Feed-Forward Network\n",
        "\n",
        "### Subtask:\n",
        "Create a simple feed-forward network, which is typically a two-layer neural network with a ReLU activation, to be used within each transformer block.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24648b3c"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires implementing a `FeedFoward` class, which will consist of two linear layers with a ReLU activation and a dropout layer, as detailed in the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b30e68a9",
        "outputId": "b54b1bdb-d4b5-43af-e022-b3ade01a2c13"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, C):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(C, 4 * C),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * C, C),\n",
        "            nn.Dropout(0.1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "print(\"FeedFoward class defined successfully.\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FeedFoward class defined successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "309eef6d"
      },
      "source": [
        "## Implement Transformer Block\n",
        "\n",
        "### Subtask:\n",
        "Assemble the Multi-Head Attention and Feed-Forward Network components, along with layer normalization and residual connections, into a complete Transformer Block.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cc3dd61"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires assembling the `MultiHeadAttention` and `FeedFoward` components into a `Block` class, including layer normalization and residual connections. I will define the `Block` class with its `__init__` and `forward` methods as per the instructions, calculating `num_heads` based on `C` and `head_size`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6f70fa27",
        "outputId": "91a6cf39-eb23-48c5-bce8-a891574066a5"
      },
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, C, head_size):\n",
        "        super().__init__()\n",
        "        num_heads = C // head_size\n",
        "        self.sa = MultiHeadAttention(num_heads, head_size)\n",
        "        self.ffwd = FeedFoward(C)\n",
        "        self.ln1 = nn.LayerNorm(C)\n",
        "        self.ln2 = nn.LayerNorm(C)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x)) # Apply LayerNorm before attention, then add residual\n",
        "        x = x + self.ffwd(self.ln2(x)) # Apply LayerNorm before feed-forward, then add residual\n",
        "        return x\n",
        "\n",
        "print(\"Block class defined successfully.\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Block class defined successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d84b1b16"
      },
      "source": [
        "## Implement GPT Model\n",
        "\n",
        "### Subtask:\n",
        "Build the full GPT model by stacking multiple Transformer Blocks, adding the initial embedding layers, and a final linear layer for language modeling head to predict the next token.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "180574ae"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires implementing the full GPT model. I will define the `GPT` class, initializing the embedding layers, stacking multiple `Block` instances, adding a final layer normalization, and a linear head for token prediction. The `forward` method will handle combining embeddings, passing data through blocks, and computing logits and loss as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfd86087",
        "outputId": "b592b219-af11-42c0-860e-4fb82b0eb657"
      },
      "source": [
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, C)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, C)\n",
        "        num_heads = C // head_size\n",
        "        self.blocks = nn.Sequential(*[Block(C, head_size) for _ in range(3)]) # Stack 3 Transformer Blocks\n",
        "        self.ln_f = nn.LayerNorm(C) # final layer norm\n",
        "        self.lm_head = nn.Linear(C, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            # Reshape logits and targets for F.cross_entropy\n",
        "            logits = logits.view(B*T, vocab_size)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "print(\"GPT class defined successfully.\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT class defined successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "929b7bbc"
      },
      "source": [
        "## Training Loop\n",
        "\n",
        "### Subtask:\n",
        "Set up the training loop, including defining the loss function (CrossEntropyLoss), an optimizer (AdamW), and iterating over batches of data to train the model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76ea54ee"
      },
      "source": [
        "**Reasoning**:\n",
        "The instructions require instantiating the GPT model, moving it to the correct device (CUDA if available, else CPU), and defining the AdamW optimizer with a specified learning rate. This code block will perform all these initial setup steps for the training loop.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54a67d18",
        "outputId": "45c05e79-c269-48a6-db9d-5ac3502927a7"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "m = GPT()\n",
        "m.to(device)\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
        "\n",
        "print(f\"Model instantiated and moved to: {device}\")\n",
        "print(f\"Optimizer created: {optimizer}\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model instantiated and moved to: cpu\n",
            "Optimizer created: AdamW (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    decoupled_weight_decay: True\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.001\n",
            "    maximize: False\n",
            "    weight_decay: 0.01\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db00a1f1"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires iterating over batches of data to train the model. This involves getting a batch, performing a forward pass, calculating loss, performing a backward pass, and updating the model parameters within a training loop.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55b92099",
        "outputId": "d14bb165-5cb3-46ea-d140-da8723989b91"
      },
      "source": [
        "epochs = 5000 # Number of training iterations\n",
        "\n",
        "for iter in range(epochs):\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # move data to the same device as the model\n",
        "    xb, yb = xb.to(device), yb.to(device)\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if iter % 500 == 0:\n",
        "        print(f\"Iteration {iter}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "print(f\"Training complete. Final Loss: {loss.item():.4f}\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0, Loss: 4.3853\n",
            "Iteration 500, Loss: 3.0012\n",
            "Iteration 1000, Loss: 2.6455\n",
            "Iteration 1500, Loss: 2.8824\n",
            "Iteration 2000, Loss: 2.7084\n",
            "Iteration 2500, Loss: 2.5716\n",
            "Iteration 3000, Loss: 2.0871\n",
            "Iteration 3500, Loss: 2.1791\n",
            "Iteration 4000, Loss: 2.1737\n",
            "Iteration 4500, Loss: 2.3662\n",
            "Training complete. Final Loss: 2.6273\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec9f2a18"
      },
      "source": [
        "## Model Generation Function\n",
        "\n",
        "### Subtask:\n",
        "Implement a function to generate text from the trained GPT model, allowing the model to sample tokens autoregressively.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcc269cb"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires implementing a `generate` function for the GPT model. This function will take an initial sequence (`idx`), and autoregressively generate `max_new_tokens` by repeatedly predicting the next token, sampling from the predictions, and appending it to the sequence. Finally, I will test the function with an example to demonstrate its functionality.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed416fba",
        "outputId": "dd4321f5-6821-4445-8896-705b1417a26e"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def generate(model, idx, max_new_tokens):\n",
        "    # idx is (B, T) array of indices in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "        # crop idx to the last block_size tokens\n",
        "        idx_cond = idx[:, -block_size:]\n",
        "        # get the predictions\n",
        "        logits, loss = model(idx_cond) # `model` refers to `m` in this context\n",
        "        # focus only on the last time step\n",
        "        logits = logits[:, -1, :] # becomes (B, C)\n",
        "        # apply softmax to get probabilities\n",
        "        probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "        # sample from the distribution\n",
        "        idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "        # append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "    return idx\n",
        "\n",
        "# Test the generation function\n",
        "# Create a starting prompt (e.g., a single newline character to start fresh)\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(generate(m, context, max_new_tokens=100)[0].tolist()))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "And.\n",
            "Carl get whe lise ordovers hes the thair word yeald tatlem lild to well that\n",
            "Tou bot and ocoye.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca79a10c"
      },
      "source": [
        "## Train and Evaluate Model\n",
        "\n",
        "### Subtask:\n",
        "Execute the training loop for a specified number of iterations and periodically evaluate the model's performance on the validation set. After training, use the model to generate a sample text.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a636151d"
      },
      "source": [
        "### Define and Integrate Evaluation Function\n",
        "\n",
        "**Reasoning**:\n",
        "The subtask requires defining an evaluation function `estimate_loss()` to assess the model's performance on both training and validation sets. This function will be integrated into the existing training loop to provide periodic updates on the model's generalization capabilities.\n",
        "\n",
        "### Generate Text After Training\n",
        "\n",
        "**Reasoning**:\n",
        "After the training is complete, the subtask requires using the trained model to generate a sample text. This will demonstrate the model's learned ability to produce new sequences based on a given context, by calling the previously defined `generate` function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8414e15c"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires defining the `estimate_loss` function. This code block will implement the function as specified, which includes setting the model to evaluation mode, calculating losses on training and validation sets over several batches, and then returning the model to training mode.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78b8f8d8",
        "outputId": "d84985f7-2919-4bb2-c092-f04af3fc8d54"
      },
      "source": [
        "eval_iters = 200 # Number of batches to average loss over for evaluation\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss(model):\n",
        "    out = {}\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            X, Y = X.to(device), Y.to(device)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train() # Set model back to training mode\n",
        "    return out\n",
        "\n",
        "print(\"estimate_loss function defined successfully.\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "estimate_loss function defined successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "681c4e2b"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires integrating the `estimate_loss` function into the training loop for periodic evaluation and then generating text after training. This code block will modify the existing training loop to include periodic calls to `estimate_loss` and will add the text generation step at the end.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "915cf03f",
        "outputId": "fad3da29-f430-471b-ddfb-526e69340102"
      },
      "source": [
        "epochs = 5000 # Number of training iterations\n",
        "\n",
        "for iter in range(epochs):\n",
        "    # evaluate loss on train and val sets periodically\n",
        "    if iter % 500 == 0 or iter == epochs - 1:\n",
        "        losses = estimate_loss(m)\n",
        "        print(f\"Iteration {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # move data to the same device as the model\n",
        "    xb, yb = xb.to(device), yb.to(device)\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True);\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(f\"\\nTraining complete after {epochs} iterations.\")\n",
        "\n",
        "# Generate text after training\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(\"\\nGenerated text:\")\n",
        "print(decode(generate(m, context, max_new_tokens=100)[0].tolist()))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0: train loss 2.2708, val loss 2.2899\n",
            "Iteration 500: train loss 2.2387, val loss 2.2567\n",
            "Iteration 1000: train loss 2.2553, val loss 2.2441\n",
            "Iteration 1500: train loss 2.2429, val loss 2.2560\n",
            "Iteration 2000: train loss 2.2325, val loss 2.2492\n",
            "Iteration 2500: train loss 2.2018, val loss 2.2340\n",
            "Iteration 3000: train loss 2.1850, val loss 2.2186\n",
            "Iteration 3500: train loss 2.1895, val loss 2.2133\n",
            "Iteration 4000: train loss 2.1788, val loss 2.1955\n",
            "Iteration 4500: train loss 2.1893, val loss 2.1923\n",
            "Iteration 4999: train loss 2.1344, val loss 2.1962\n",
            "\n",
            "Training complete after 5000 iterations.\n",
            "\n",
            "Generated text:\n",
            "\n",
            "Will.\n",
            "\n",
            "Switk I kust ould mang mousbedes ot if boster:\n",
            "When coutenst Fighen, your frieasu and thred s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ee0bbfb"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   **Embedding Layers**: The model successfully initialized token embeddings (`token_embedding_table`) with a shape of `[65, 32]` (vocab\\_size, embedding\\_dimension) and positional embeddings (`position_embedding_table`) with a shape of `[8, 32]` (block\\_size, embedding\\_dimension). The vocabulary size was determined to be 65.\n",
        "*   **Transformer Components**:\n",
        "    *   A single self-attention `Head` was implemented with query, key, and value linear layers and a masked attention mechanism.\n",
        "    *   The `MultiHeadAttention` module was successfully created, combining multiple `Head` instances and applying a final linear projection.\n",
        "    *   A `FeedFoward` network was implemented as a two-layer neural network with ReLU activation and dropout.\n",
        "    *   These components were integrated into a `Block` module, including layer normalization and residual connections, to form a complete Transformer block.\n",
        "*   **GPT Model Construction**: The full `GPT` model was assembled by stacking 3 Transformer `Block` instances, along with the initial token and positional embedding layers, a final layer normalization, and a linear language modeling head for next-token prediction.\n",
        "*   **Training Setup**:\n",
        "    *   The `GPT` model was instantiated and moved to the 'cpu' device.\n",
        "    *   An `AdamW` optimizer was configured with a learning rate of $1e-3$.\n",
        "*   **Training Progress (Initial Loop)**: After 5000 training iterations, the loss decreased from an initial value of $4.3853$ to a final value of $2.6273$, indicating that the model was learning.\n",
        "*   **Evaluation Integration**: An `estimate_loss` function was successfully defined and integrated into the training loop to periodically evaluate the model's performance on both training and validation sets. At iteration 0, the training loss was $2.2708$ and validation loss was $2.2899$. At iteration 1000, these had decreased to $2.2553$ (train) and $2.2441$ (val), demonstrating continued learning.\n",
        "*   **Text Generation**: A `generate` function was implemented, allowing the trained `GPT` model to produce autoregressive text sequences of a specified length (e.g., 100 new tokens) based on an initial context.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The foundational architecture of a GPT-like transformer model has been successfully implemented and partially trained, demonstrating the core components are functional.\n",
        "*   To further improve model performance and generated text quality, comprehensive hyperparameter tuning (e.g., learning rate, number of blocks, head size, dropout rates) and training on a larger, more diverse dataset are essential next steps.\n"
      ]
    }
  ]
}